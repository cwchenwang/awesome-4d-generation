# Awesome 4D Generation
This repo collects papers for 4D generation.

## Table of Contents
- [Camera Control for Video Diffusion](#camera-control-for-video-diffusion)
- [Multi-view for Video Diffusion](#multi-view-for-video-diffusion)
- [Distillation from Video Diffusion](#distillation-from-video-diffusion)
- [Generation by Reconstruction](#generation-by-reconstruction)
- [4D Editing](#4d-editing)
- [Physics](#physics)

## Camera Control for Video Diffusion

VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control

[ğŸ“„ Paper](https://arxiv.org/abs/2407.12781) | [ğŸŒ Project Page](https://snap-research.github.io/vd3d/)

Controlling Space and Time with Diffusion Models

[ğŸ“„ Paper](https://arxiv.org/pdf/2407.07860) | [ğŸŒ Project Page](https://4d-diffusion.github.io/)

CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2406.02509) | [ğŸŒ Project Page](https://ir1d.github.io/CamCo/)

Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control

[ğŸ“„ Paper](https://arxiv.org/pdf/2405.17414) | [ğŸŒ Project Page](https://collaborativevideodiffusion.github.io/)

## Multi-view for Video Diffusion

SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency, Xie et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/pdf/2407.17470) | [ğŸŒ Project Page](https://sv4d.github.io/) | [ğŸ’» Code](https://github.com/Stability-AI/generative-models)

L4GM: Large 4D Gaussian Reconstruction Model, Ren et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/abs/2406.10324) | [ğŸŒ Project Page](https://research.nvidia.com/labs/toronto-ai/l4gm/)

4Diffusion: Multi-view Video Diffusion Model for 4D Generation, Zhang et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/pdf/2405.20674) | [ğŸŒ Project Page](https://aejion.github.io/4diffusion) | [ğŸ’» Code](https://github.com/aejion/4Diffusion) 

Diffusion4D: Fast Spatial-temporal Consistent 4D Generation via Video Diffusion Models, Liang et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/abs/2405.16645) | [ğŸŒ Project Page](https://vita-group.github.io/Diffusion4D/) | [ğŸ’» Code](https://github.com/VITA-Group/Diffusion4D) | [ğŸ¥ Video](https://www.youtube.com/watch?v=XJT-cMt_xVo)

## Distillation from Video Diffusion

4Dynamic: Text-to-4D Generation with Hybrid Priors, Yuan et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/abs/2407.12684)

STAR: Skeleton-aware Text-based 4D Avatar Generation with In-Network Motion Retargeting, Chai et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/abs/2406.04629) | [ğŸŒ Project Page](https://star-avatar.github.io/) | [ğŸ’» Code](https://github.com/czh-98/STAR)

MotionDreamer: Zero-Shot 3D Mesh Animation from Video Diffusion Models, Uzolas et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/pdf/2405.20155) | [ğŸ’» Code](https://github.com/lukasuz/MotionDreamer)

PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting, Miao et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/pdf/2405.19957) | [ğŸŒ Project Page](https://github.com/MiaoQiaowei/PLA4D.github.io)

MagicPose4D: Crafting Articulated Models with Appearance and Motion Control, Zhang et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/pdf/2405.14017) | [ğŸŒ Project Page](https://boese0601.github.io/magicpose4d/) | [ğŸ’» Code](https://github.com/haoz19/MagicPose4D) 

SC4D: Sparse-Controlled Video-to-4D GenerationÂ and Motion Transfer, Wu et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/abs/2404.03736) | [ğŸŒ Project Page](https://sc4d.github.io/) | [ğŸ’» Code](https://github.com/JarrentWu1031/SC4D) | [ğŸ¥ Video](https://www.youtube.com/watch?v=SkpTEuX4B5c)

TC4D: Trajectory-Conditioned Text-to-4D Generation, Bahmani et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/pdf/2403.17920) | [ğŸŒ Project Page](https://sherwinbahmani.github.io/tc4d) | [ğŸ’» Code](https://github.com/sherwinbahmani/tc4d)

Comp4D: LLM-Guided Compositional 4D Scene Generation, Xu et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/abs/2403.16993) | [ğŸŒ Project Page](https://vita-group.github.io/Comp4D/) | [ğŸ’» Code](https://github.com/VITA-Group/Comp4D) | [ğŸ¥ Video](https://www.youtube.com/watch?v=9q8SV1Xf_Xw)

STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians, Zetn et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/pdf/2403.14939.pdf) | [ğŸŒ Project Page](https://nju-3dv.github.io/projects/STAG4D/) | [ğŸ’» Code](https://github.com/zeng-yifei/STAG4D) 

GaussianFlow:Â Splatting GaussianÂ Dynamics forÂ 4DÂ Content Creation, Gao et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/abs/2403.12365) | [ğŸŒ Project Page](https://zerg-overmind.github.io/GaussianFlow.github.io/) | [ğŸ’» Code](https://github.com/Zerg-Overmind/GaussianFlow)

4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency, Yin et al., Arxiv 2023

[ğŸ“„ Paper](https://arxiv.org/pdf/2312.17225) | [ğŸŒ Project Page](https://vita-group.github.io/4DGen/) | [ğŸ’» Code](https://github.com/VITA-Group/4DGen) | [ğŸ¥ Video](https://www.youtube.com/watch?v=-bXyBKdpQ1o)

DreamGaussian4D: Generative 4D Gaussian Splatting, Ren et al., CVPR 2024

[ğŸ“„ Paper](https://arxiv.org/pdf/2312.13763) | [ğŸŒ Project Page](https://jiawei-ren.github.io/projects/dreamgaussian4d/)

Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models, Ling et al., Arxiv 2023

[ğŸ“„ Paper](https://arxiv.org/pdf/2312.03795) | [ğŸŒ Project Page](https://animatabledreamer.github.io/) | [ğŸ’» Code](https://github.com/AnimatableDreamer/AnimatableDreamer)

AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation and Reconstruction with Canonical Score Distillation, Wang et al., Arxiv 2023

[ğŸ“„ Paper](https://arxiv.org/pdf/2312.03795) | [ğŸŒ Project Page](https://animatabledreamer.github.io/) | [ğŸ’» Code](https://github.com/AnimatableDreamer/AnimatableDreamer)

4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling, Bahmani et al., CVPR 2024

[ğŸ“„ Paper](https://arxiv.org/pdf/2311.17984) | [ğŸŒ Project Page](https://research.nvidia.com/labs/nxp/dream-in-4d/) | [ğŸ’» Code](https://github.com/sherwinbahmani/4dfy)

A Unified Approach for Text- and Image-guided 4D Scene Generation, Zheng et al., CVPR 2024

[ğŸ“„ Paper](https://arxiv.org/pdf/2311.17984) | [ğŸŒ Project Page](https://sherwinbahmani.github.io/4dfy) | [ğŸ’» Code](https://github.com/NVlabs/dream-in-4d)

Animate124: Animating One Image to 4D Dynamic Scene, Zhao et al., Arxiv 2023

[ğŸ“„ Paper](https://arxiv.org/pdf/2311.14603) | [ğŸŒ Project Page](https://animate124.github.io/) | [ğŸ’» Code](https://github.com/HeliosZhao/Animate124)

Consistent4D: Consistent 360Â° Dynamic Object Generation from Monocular Video, Jiang et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/pdf/2311.02848) | [ğŸŒ Project Page](https://consistent4d.github.io/) | [ğŸ’» Code](https://github.com/yanqinJiang/Consistent4D)

Text-To-4D Dynamic Scene Generation, Singer et al., Arxiv 2023

[ğŸ“„ Paper](https://arxiv.org/pdf/2301.11280) | [ğŸŒ Project Page](https://make-a-video3d.github.io)

## Generation by Reconstruction

4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models, Yu et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/abs/2406.07472) | [ğŸŒ Project Page](https://snap-research.github.io/4Real/)

Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels, Wang et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/abs/2405.16822) | [ğŸŒ Project Page](https://vidu4d-dgs.github.io/)

## 4D Editing

Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D Diffusion, Mou et al., CVPR 2024

[ğŸ“„ Paper](https://arxiv.org/abs/2406.09402) | [ğŸŒ Project Page](https://immortalco.github.io/Instruct-4D-to-4D/)

## Physics

Sync4D: Video Guided Controllable Dynamics for Physics-Based 4D Generation, Fu et al., Arxiv 2024

[ğŸ“„ Paper](https://arxiv.org/abs/2405.16849) | [ğŸŒ Project Page](https://sync4dphys.github.io/)
